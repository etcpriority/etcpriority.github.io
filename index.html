<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Learning to Prioritize: A Novel Algorithm for Learning Event-Triggered Control in Decentralized Multi-Agent Systems">
  <meta name="keywords" content="Muti-Agent System, ETC">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Learning to Prioritize: A Novel Algorithm for Learning Event-Triggered Control in Decentralized Multi-Agent Systems</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Learning to Prioritize: A Novel Algorithm for Learning Event-Triggered Control in Decentralized Multi-Agent Systems</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Qingyun Guo<sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.aalto.fi/en/persons/junyi-shi/publications/">Junyi Shi</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.aalto.fi/en/persons/tomasz-kucner/publications">Tomasz Piotr Kucner</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://baumanndominik.github.io/">Dominik Baumann</a><sup>1,2,3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Aalto University</span>
            <span class="author-block"><sup>2</sup>Finnish Center for Artificial Intelligence</span>
            <span class="author-block"><sup>3</sup>Uppsala University</span>
          </div> 
          
<!--           <div class="column has-text-centered"> -->
<!--             <div class="publication-links"> -->
              <!-- PDF Link. -->
<!--               <span class="link-block"> -->
<!--                 <a href="https://arxiv.org/" class="external-link button is-normal is-rounded is-dark"> -->
<!--                   <span class="icon"> -->
<!--                     <i class="fas fa-file-pdf"></i> -->
<!--                   </span> -->
<!--                   <span>Paper</span> -->
<!--                 </a> -->
<!--               </span> -->
<!--               <span class="link-block"> -->
<!--                 <a href="https://arxiv.org/" class="external-link button is-normal is-rounded is-dark"> -->
<!--                   <span class="icon"> -->
<!--                     <i class="ai ai-arxiv"></i> -->
<!--                   </span> -->
<!--                   <span>arXiv</span> -->
<!--                 </a> -->
<!--               </span> -->
<!--             </div> -->
<!--           </div> -->
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <h2 class="title is-4">Abstract</h2>
    <div class="content has-text-justified">
      <p>
        In multi-agent systems, effective communication between agents is essential for 
        coordination and control, yet communication resources are often limited, posing 
        challenges to maintaining system performance. Event-triggered control offers a 
        solution by initiating communication only when necessary, thus reducing unnecessary 
        data transmission.
      </p>
      <p>  
        However, current multi-agent reinforcement learning algorithms 
        for event-triggered control face two significant limitations. First, they rely on 
        the centralized training and decentralized execution paradigm, which requires global 
        information from periodic communication during training---a requirement that can often 
        not be met in real-world applications. Second, these methods consider discrete, 
        binary communication decisions. This adds complexity to the learning process as 
        the action space is now hybrid: continuous control commands and discrete communication 
        decisions. Further, binary communication decisions may still result in more agents 
        communicating than the network bandwidth can support. 
      </p>
      <p> 
        In this paper, we propose a novel algorithm for learning event-triggered control that considers decentralized systems and 
        uses only local observations and communicated information throughout the training and 
        execution phases. Moreover, our algorithm simplifies the action space by allowing agents 
        to jointly learn continuous control policies and communication priorities. We validate 
        our method across various multi-agent learning tasks with communication interference, 
        demonstrating its superiority over the baseline approach.
      </p>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Overview</h2>
    <!-- Flex container for centering the image -->
    <div style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
      <!-- Image to illustrate your method -->
      <div style="text-align: center; width: 500px;"> 
        <img src="./static/images/system.png" alt="Description of the method" style="max-width: 100%; height: auto;" />
      </div>
      <div class="content has-text-justified">
        <p>
          Our method jointly learns control policies and communication priorities using the 
          independent proximal policy optimization algorithm. A decentralized multi-agent system where each agent \(i\) learns the policy \(\pi(a_i | o_i, \hat{o})\) 
          based on local observations \(o_i\) and communicated observations \(\hat{o}\). 
          Agent \(i\) exchanges the communication priority \(a_{i,\mathrm{p}}\) through a wireless network. 
          Moreover, \(L\) agents with the highest priorities are allocated bandwidth and send crucial information \(\hat{o}\) to other agents. 
          Agent \(i\) executes the control action \(a_{i,c}\) to interact with the environment, getting corresponding rewards and local observations, 
          which feed back into the policy learning.
        </p>
      </div>
    </div>
  </div>
</section>




<section class="section">
      <div class="container is-max-desktop">
      <h2 class="title is-4">Coverage Control (Three Agents)</h2>
      <div class="content has-text-justified">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <video id="spread_p_3_t3e4" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/spread_p_3_t3e4.mp4" type="video/mp4">
          </video>
          <p>(a) Ours</p>
        </div>
        <div class="column has-text-centered">
          <video id="spread_r_3_t2e4" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/spread_r_3_t2e4.mp4" type="video/mp4">
          </video>
          <p>(b) Round-Robin Communication</p>
        </div>
      </div>
      <p>
        In coverage control, our algorithm enables agents to quickly coordinate and 
        cover the corresponding landmarks. In the final performance, two agents are nearly 
        aligned with their respective landmarks, while one 
        agent has a short distance away from its landmark.
      </p>
      <p>
        The policy learned by the baseline lets the agents go to different landmarks at the initial stage, 
        but in the middle, two of the agents show a lack of awareness regarding which landmark 
        they are supposed to cover. In the end, none of the three agents can be fully 
        aligned with landmarks.
      </p>
      </div>
      </div>
</section>
    
<section class="section"> 
  <div class="container is-max-desktop">
      <h2 class="title is-4">Formation Control (Eight Agents)</h2>
      <div class="content has-text-justified">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <video id="render_for_8_p (2)" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/render_for_8_p (2).mp4" type="video/mp4">
          </video>
          <p>(a) Ours</p>
        </div>
        <div class="column has-text-centered">
          <video id="render_for_8_r" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/render_for_8_r.mp4" type="video/mp4">
          </video>
          <p>(b) Round-Robin Communication</p>
        </div>
      </div>
      <p>
        In formation control, following the policy from our algorithm, the agents coordinate 
        and eventually surround the landmark, forming an approximate regular polygon.
      </p>
      <p>
        The policy from the baseline fails to allow the agents to be distributed in a 
        desired manner around the landmark. Some agents place themselves in appropriate 
        locations, while the remaining agents do not reach the corresponding locations.
      </p>
      </div>
      </div>
 </section>



    <section class="section">
      <div class="container is-max-desktop">
        <h2 class="title is-4">Multiwalker</h2>
        <!-- Flex container for centering the videos in two columns -->

          <h2 class="title is-5">Three Agents</h2>
        <div class="container is-max-desktop">
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <div class="column has-text-centered">
                <video id="walker_3_p_t6e2"autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/walker_3_p_t6e2.mp4" type="video/mp4">
                </video>
                <p>(a) Ours</p>
              </div>
              <div class="column has-text-centered">
                <video id="walker_3_r_t20e6"autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/walker_3_r_t20e6.mp4" type="video/mp4">
                </video>
                <p>(b) Round-Robin Communication</p>
              </div>
            </div>
            <p>
              Under limited bandwidth conditions, the agents trained with our algorithm initially adopt a simplified policy, 
              passing the package to the rightmost agent to minimize complex coordination, which would 
              otherwise require more communication. The rightmost agent stabilizes the package by 
              positioning itself at its center of mass, balancing it effectivelyâ€”even performing 
              jumps to maintain stability. Eventually, however, the package falls to the ground due 
              to risky balancing and an unexpected terrain configuration not encountered 
              in training. Overall, our method enables agents to transport packages over considerable 
              distances, despite communication constraints.
            </p>
            <p>
              With the baseline, the agents adopt a similar policy, that is, let as few agents as possible transport the package. However, 
              around the midpoint, the middle agent approaches the rightmost one, aiming to 
              transfer the package. Here, a slight misalignment occurs due to the lack of the important information, with the middle agent moving out 
              of sync with the rightmost agent's reach. This desynchronization leads to the package 
              slipping, resulting in it falling to the ground.
            </p>
          </div>
    
        <div class="container is-max-desktop">
          <h2 class="title is-5">Six Agents</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <div class="column has-text-centered">
                <video id="walker_6_p_t21e8"autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/walker_6_p_t21e8.mp4" type="video/mp4">
                </video>
                <p>(a) Ours</p>
              </div>
              <div class="column has-text-centered">
                <video id="walker_r_6_t4e7" autoplay controls muted loop playsinline width="100%">
                  <source src="./static/videos/walker_r_6_t4e7.mp4" type="video/mp4">
                </video>
                <p>(b) Round-Robin Communication</p>
              </div>
            </div>
            <p>
              When the number of agents increases to six, the policy learned by our algorithm is 
              to pass the package to the four agents on the right, who then transport it over a 
              considerable distance. However, because the agents are so close together and lack 
              sufficient communication, the leftmost agent eventually steps on the second agent 
              from the left.
            </p>
            <p>
              With six agents, the policy obtained from the baseline is still to let as few agents as 
              possible transport the package. However, this approach introduces certain risks: 
              some middle agents exhibit less effort during transfers and fail to recognize how 
              their forward movement impacts the overall task. As a result, they end up tripping 
              other agents, and the package does not travel very far.
            </p>
          </div>
        </div>
        
    
      
      </div>
    </section>
    
    
    

<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The website design is a modification of <a href="https://nerfies.github.io">Nerfies</a>.  Thanks to them for their designing and open sourcing this template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
